{
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0K9I-vrCi806"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "# !pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes\n",
        "\n",
        "# cuda 12.1 version\n",
        "!pip3 install -U xformers --index-url https://download.pytorch.org/whl/cu121\n",
        "\n",
        "!pip install torch sentence-transformers\n",
        "!pip install PyPDF2 triton\n",
        "!pip3 install torchvision torchaudio"
      ],
      "metadata": {
        "id": "wnpyTX_7CAkn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n"
      ],
      "metadata": {
        "id": "dzp_4o--CD6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 4bit pre quantized models unsloth support for 4x faster downloading + no OOMs.\n",
        "fourbit_models = [\n",
        "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 15 trillion tokens model 2x faster!\n",
        "    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
        "    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
        "    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # We also uploaded 4bit for 405b!\n",
        "    \"unsloth/Mistral-Nemo-Base-2407-bnb-4bit\", # New Mistral 12b 2x faster!\n",
        "    \"unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit\",\n",
        "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",        # Mistral v3 2x faster!\n",
        "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
        "    \"unsloth/Phi-3-mini-4k-instruct\",          # Phi-3 2x faster!d\n",
        "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
        "    \"unsloth/gemma-2-9b-bnb-4bit\",\n",
        "    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n",
        "] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")"
      ],
      "metadata": {
        "id": "ohXwlTBHCIDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")\n"
      ],
      "metadata": {
        "id": "x_nT8zBlCIAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Define your chat_template\n",
        "chat_template = \"\"\"system\n",
        "\n",
        "{SYSTEM}user\n",
        "\n",
        "{INPUT}assistant\n",
        "\n",
        "{OUTPUT}\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token  # Must add EOS_TOKEN\n",
        "\n",
        "# Define your formatting function\n",
        "def formatting_prompts_func(examples):\n",
        "    systems = examples[\"system\"]  # Assuming you have a \"system\" field in your data\n",
        "    inputs = examples[\"input\"]\n",
        "    outputs = examples[\"output\"]\n",
        "    texts = []\n",
        "    for system, input, output in zip(systems, inputs, outputs):\n",
        "        text = chat_template.format(SYSTEM=system, INPUT=input, OUTPUT=output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return {\"text\": texts}\n",
        "\n",
        "# Load your custom dataset\n",
        "dataset_dict = load_dataset(\"json\", data_files=\"/content/Training_dataset.json\")\n",
        "\n",
        "# Select the 'train' split\n",
        "dataset = dataset_dict[\"train\"]\n",
        "\n",
        "# Apply the formatting function\n",
        "dataset = dataset.map(formatting_prompts_func, batched=True)\n",
        "\n",
        "\n",
        "# # ------------------------------------------------------------------------------------------------\n",
        "# [\n",
        "#   {\n",
        "#     \"system\": \"You are a helpful assistant.\",\n",
        "#     \"input\": \"What is the capital of France?\",\n",
        "#     \"output\": \"The capital of France is Paris.\"\n",
        "#   },\n",
        "#   {\n",
        "#     \"system\": \"You are an expert in geography.\",\n",
        "#     \"input\": \"Name the largest desert in the world.\",\n",
        "#     \"output\": \"The largest desert in the world is the Antarctic Desert.\"\n",
        "#   }\n",
        "# ]\n"
      ],
      "metadata": {
        "id": "FliQovTdCH9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False, # Can make training 5x faster for short sequences.\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        num_train_epochs = 6,  # Set this for 1 full training run.\n",
        "        max_steps = 60,\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        # evaluation_strategy = \"steps\",  # Evaluate at each logging step\n",
        "        # eval_steps = 10,  # Evaluate every 10 steps (you can adjust this value)\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        logging_dir = \"logs\",  # Directory to save logs\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "st4ewUC3CQ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#@title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ],
      "metadata": {
        "id": "mIYunYqICQ4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "trainer_stats = trainer.train()"
      ],
      "metadata": {
        "id": "oHOR-2ZUCQ05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(trainer_stats)"
      ],
      "metadata": {
        "id": "L7LfUsUQOz0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory         /max_memory*100, 3)\n",
        "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")\n"
      ],
      "metadata": {
        "id": "LKwu2vGDCQxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# system_text = \"You are an AI assistant. give the anser for the Question from the Following given context\"\n",
        "\n",
        "# FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "# inputs = tokenizer(\n",
        "# [\n",
        "#     chat_template.format(\n",
        "#             SYSTEM=system_text,  # Provide the SYSTEM value\n",
        "#             INPUT=\"context: Management of brain metastasis from rectal cancer using whole‑brain radiation therapy followed by bevacizumab and chemotherapy: A case report HUNG VAN NGUYEN1,2, DUONG THUY PHUNG2, TRUNG THANH NGUYEN2, BACH TRUNG TRAN2 , KIM NGAN THI MAI1,2 and HUY LE TRINH1,2 1 Department of Oncology and Palliative Care, Hanoi Medical University Hospital;  2 Department of Oncology, Hanoi Medical University, Hanoi 100000, Vietnam Received February 7, 2023; Accepted July 17, 2023.  Question: what is the address of primary author.\",\n",
        "#             OUTPUT=\"\",  # This is the empty output in the template\n",
        "#     )\n",
        "# ], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "# outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n",
        "# decoded_outputs = tokenizer.batch_decode(outputs)\n",
        "\n",
        "# print(decoded_outputs)"
      ],
      "metadata": {
        "id": "1Dmc4NrCCVBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the SYSTEM text\n",
        "system_text = \"You are an AI assistant. give the anser for the Question from the Following given context\"\n",
        "\n",
        "\n",
        "# Use the new context and question as the INPUT\n",
        "inputs = tokenizer(\n",
        "    [\n",
        "        chat_template.format(\n",
        "            SYSTEM=system_text,  # SYSTEM value\n",
        "            INPUT=\"context:-Management of brain metastasis from rectal cancer using whole‑brain radiation therapy followed by bevacizumab and chemotherapy: A case report HUNG VAN NGUYEN1,2, DUONG THUY PHUNG2, TRUNG THANH NGUYEN2, BACH TRUNG TRAN2 , KIM NGAN THI MAI1,2 and HUY LE TRINH1,2 1 Department of Oncology and Palliative Care, Hanoi Medical University Hospital;  2 Department of Oncology, Hanoi Medical University, Hanoi 100000, Vietnam Received February 7, 2023; Accepted July 17, 2023.  Question:- what is the address of primary author, please provide only one address. \",\n",
        "            OUTPUT=\"\",  # Leave OUTPUT blank for generation\n",
        "        )\n",
        "    ],\n",
        "    return_tensors=\"pt\"\n",
        ").to(\"cuda\")\n",
        "\n",
        "# Enable native 2x faster inference\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# Initialize TextStreamer for live streaming of generated text\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "\n",
        "# Generate the response from the model\n",
        "_ = model.generate(**inputs, streamer=text_streamer, max_new_tokens=128)\n",
        "print(_)\n"
      ],
      "metadata": {
        "id": "XI4fr77KAn0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge to 16bit\n",
        "if True: model.save_pretrained_merged(\"llama_3_1_8b_Instruct_model\", tokenizer, save_method = \"merged_16bit\",)\n",
        "\n",
        "# # Merge to 16bit\n",
        "# if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
        "# if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n",
        "\n",
        "# # Merge to 4bit\n",
        "# if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\n",
        "# if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n"
      ],
      "metadata": {
        "id": "XNAjRqFNGImb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download model using rcloud"
      ],
      "metadata": {
        "id": "BxOdzoebhPlv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Copying the finetuned model to Drive**"
      ],
      "metadata": {
        "id": "JgeluePD8lJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "vM1LxLnh6i-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "# Source and destination paths\n",
        "source_dir = \"/content/llama_3_1_8b_Instruct_model\"\n",
        "destination_dir = \"/content/drive/MyDrive/Llama 3.1 8b Instruct finetuning/Finetuned Model\"\n",
        "\n",
        "# List all items in the /content directory\n",
        "items = os.listdir(source_dir)\n",
        "\n",
        "# Iterate through all items and copy them to the destination directory\n",
        "for item in items:\n",
        "    # Skip the 'drive' folder to avoid copying it\n",
        "    if item != \"drive\":\n",
        "        # Construct full file or folder path\n",
        "        source_path = os.path.join(source_dir, item)\n",
        "        destination_path = os.path.join(destination_dir, item)\n",
        "\n",
        "        # Check if the item is a file or a directory\n",
        "        if os.path.isdir(source_path):\n",
        "            # Copy the directory and its contents\n",
        "            shutil.copytree(source_path, destination_path)\n",
        "        else:\n",
        "            # Copy the file\n",
        "            shutil.copy2(source_path, destination_path)\n",
        "\n",
        "print(\"Files and folders copied successfully!\")\n"
      ],
      "metadata": {
        "id": "XWAgzzHYGIkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QIJ2IkU7NuBO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RUN MODEL LOCALLY**"
      ],
      "metadata": {
        "id": "1aL2ArA8NvP_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "8NU_O-ZHN6X_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes\n",
        "\n",
        "# cuda 12.1 version\n",
        "!pip3 install -U xformers --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip3 install torchvision torchaudio\n",
        "!pip install torch sentence-transformers\n",
        "!pip install PyPDF2"
      ],
      "metadata": {
        "id": "lpLJPiTwOfH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "from transformers import TextStreamer\n"
      ],
      "metadata": {
        "id": "PtIBnadWGIho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load the locally saved model and tokenizer\n",
        "model_path = \"/content/drive/MyDrive/Llama 3.1 8b Instruct finetuning/Finetuned Model\"\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(model_path)\n"
      ],
      "metadata": {
        "id": "d8VnZ0rXOYQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Prepare the input for the model\n",
        "system_text = \"\"\"You are an AI assistant. Provide the answer to the question based on the given context.\"\"\"\n",
        "chat_template = \"\"\"system\n",
        "\n",
        "{SYSTEM}user\n",
        "\n",
        "{INPUT}assistant\n",
        "\n",
        "{OUTPUT}\"\"\"\n",
        "input_text = chat_template.format(\n",
        "    SYSTEM=system_text,\n",
        "    INPUT=\"context: Management of brain metastasis from rectal cancer using whole‑brain radiation therapy followed by bevacizumab and chemotherapy: A case report HUNG VAN NGUYEN1,2, DUONG THUY PHUNG2, TRUNG THANH NGUYEN2, BACH TRUNG TRAN2 , KIM NGAN THI MAI1,2 and HUY LE TRINH1,2 1 Department of Oncology and Palliative Care, Hanoi Medical University Hospital;  2 Department of Oncology, Hanoi Medical University, Hanoi 100000, Vietnam Received February 7, 2023; Accepted July 17, 2023. Question: What is the address of the primary author?\",\n",
        "    OUTPUT=\"\"  # Leave OUTPUT blank for generation\n",
        ")\n",
        "\n",
        "inputs = tokenizer([input_text], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "# Enable native 2x faster inference\n",
        "FastLanguageModel.for_inference(model)\n",
        "# Initialize TextStreamer for live streaming of generated text (optional)\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "# Generate the response from the model\n",
        "outputs = model.generate(**inputs, max_new_tokens=128, use_cache=True)\n",
        "# Decode the output\n",
        "decoded_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "# Print the result\n",
        "print(decoded_outputs)"
      ],
      "metadata": {
        "id": "LG21XLG6OQ9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import PyPDF2\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer, util"
      ],
      "metadata": {
        "id": "YtA9ROs6e1cm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "gwJSm8CtG4W7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to read and extract text from a PDF file\n",
        "def read_pdf(pdf_path):\n",
        "    with open(pdf_path, 'rb') as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        text = \"\"\n",
        "        for page in range(len(reader.pages)):\n",
        "            text += reader.pages[page].extract_text()\n",
        "    return text\n",
        "\n",
        "pdf_path = \"/content/38022102.pdf\"\n",
        "pdf_text = read_pdf(pdf_path)\n"
      ],
      "metadata": {
        "id": "IDKAz4tZB1ku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chunk_text_with_overlap(text, max_chunk_size=750, overlap_size=100):\n",
        "    \"\"\"\n",
        "    Chunk text with overlapping sentences.\n",
        "\n",
        "    :param text: The input text to be chunked.\n",
        "    :param max_chunk_size: Maximum number of tokens per chunk.\n",
        "    :param overlap_size: Number of tokens to overlap between chunks.\n",
        "    :return: List of text chunks with overlap.\n",
        "    \"\"\"\n",
        "    sentences = sent_tokenize(text)\n",
        "    chunks = []\n",
        "    current_chunk = []\n",
        "    current_length = 0\n",
        "    total_sentences = len(sentences)\n",
        "    sentence_index = 0\n",
        "\n",
        "    while sentence_index < total_sentences:\n",
        "        # Add sentences to the current chunk\n",
        "        while sentence_index < total_sentences and current_length + len(sentences[sentence_index].split()) <= max_chunk_size:\n",
        "            current_chunk.append(sentences[sentence_index])\n",
        "            current_length += len(sentences[sentence_index].split())\n",
        "            sentence_index += 1\n",
        "\n",
        "        # Append the current chunk to the list\n",
        "        chunks.append(\" \".join(current_chunk))\n",
        "\n",
        "        # Reset for the next chunk\n",
        "        current_chunk = []\n",
        "        current_length = 0\n",
        "\n",
        "        # Add overlap\n",
        "        overlap_count = min(overlap_size, total_sentences - sentence_index)\n",
        "        for _ in range(overlap_count):\n",
        "            if sentence_index < total_sentences:\n",
        "                current_chunk.append(sentences[sentence_index])\n",
        "                current_length += len(sentences[sentence_index].split())\n",
        "                sentence_index += 1\n",
        "\n",
        "    return chunks\n",
        "\n",
        "# Example usage\n",
        "chunks = chunk_text_with_overlap(pdf_text, max_chunk_size=750, overlap_size=100)\n"
      ],
      "metadata": {
        "id": "O61nuNDcB1hB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_new_tokens=300"
      ],
      "metadata": {
        "id": "2AHp2YJbB1aG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_context(chunks,query):\n",
        "  # Load a pre-trained sentence transformer model\n",
        "  embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "  # Generate embeddings for the chunks\n",
        "  chunk_embeddings = embedder.encode(chunks, convert_to_tensor=True)\n",
        "\n",
        "  # Generate embedding for the query\n",
        "  query_embedding = embedder.encode(query, convert_to_tensor=True)\n",
        "\n",
        "  # Compute cosine similarities between the query and all chunks\n",
        "  cosine_scores = util.pytorch_cos_sim(query_embedding, chunk_embeddings)\n",
        "\n",
        "  # Find the most similar chunk (highest cosine similarity score)\n",
        "  top_k = 2  # You can retrieve more chunks by increasing this value\n",
        "  top_results = torch.topk(cosine_scores, k=top_k)\n",
        "\n",
        "  # Extract the most relevant chunk(s)\n",
        "  relevant_chunks = [chunks[idx] for idx in top_results[1][0]]\n",
        "\n",
        "  # Join the relevant chunks if needed\n",
        "  context = \" \".join(relevant_chunks)\n",
        "  return context\n",
        "\n",
        "def result(input_text=input_text, model=model,tokenizer=tokenizer,max_new_tokens=max_new_tokens):\n",
        "  inputs = tokenizer([input_text], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "  # Enable native 2x faster inference\n",
        "  FastLanguageModel.for_inference(model)\n",
        "\n",
        "  # Initialize TextStreamer for live streaming of generated text (optional)\n",
        "  text_streamer = TextStreamer(tokenizer)\n",
        "\n",
        "  # Generate the response from the model\n",
        "  outputs = model.generate(**inputs, max_new_tokens= max_new_tokens,use_cache=True)\n",
        "\n",
        "  # Decode the output\n",
        "  decoded_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "  return decoded_outputs\n"
      ],
      "metadata": {
        "id": "29MxkdbmGZ5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Primary Author Address**"
      ],
      "metadata": {
        "id": "bg9_GKgIJnDQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is the address of the Primart Author?\"\n",
        "\n",
        "## Prepare the input with a selected chunk of text\n",
        "context = get_context(chunks, query)\n",
        "# context = pdf_text\n",
        "\n",
        "# System text for LLaMA 3.1 8B model\n",
        "system_text = \"\"\"\n",
        "You are a literature analyst with the ability to understand the provided context and figure out the appropriate answers for the asked queries from the context only.\n",
        "It's important to give accurate answers without making assumptions.\n",
        "\n",
        "Rules:\n",
        "1. Do not generate information not present in the context.\n",
        "2. Follow the response format very strictly.\n",
        "3. Do not display any thought process in the response.\n",
        "4. Focus on author affiliations and contributions for the address. Prioritize addresses starting with 1, A, or I.\n",
        "5. If multiple addresses are present, use the first address as the primary author's address, and Strictly dont give rest all addresses in the responce.\n",
        "6. If a city or state is mentioned, deduce the corresponding country.\n",
        "7. Please dont print the context in the Responce.\n",
        "8. Response Format: - Primary Author's Address: [Address]\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Formatting the input for the model\n",
        "chat_template = \"\"\"system\n",
        "\n",
        "{SYSTEM}user\n",
        "\n",
        "{INPUT}assistant\n",
        "\n",
        "{OUTPUT}\"\"\"\n",
        "\n",
        "input_text = chat_template.format(\n",
        "    SYSTEM=system_text,\n",
        "    INPUT=f\"context: {context}\\nQuestion: {query}\\n\",\n",
        "    OUTPUT=\"\"\n",
        ")\n",
        "max_new_tokens=300\n",
        "# Generate the response from the model\n",
        "primary_author_address = result(input_text, model, tokenizer,max_new_tokens)\n",
        "\n",
        "# Print the result\n",
        "print(primary_author_address[0])\n"
      ],
      "metadata": {
        "id": "ktuIlEEG3yVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure it's a string by accessing the first element if it's a list\n",
        "if isinstance(primary_author_address, list):\n",
        "    primary_author_address = primary_author_address[0]\n",
        "\n",
        "\n",
        "# Split the string based on the word \"assistant\"\n",
        "address = primary_author_address.split(\"assistant\")\n",
        "address = address[1].split(\"2.\")"
      ],
      "metadata": {
        "id": "8hJr6BtYPNYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(address[0])"
      ],
      "metadata": {
        "id": "Tg-kNnwJSSot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Patient Details**"
      ],
      "metadata": {
        "id": "YkNWkX2CKPFG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drug=\"Hydralazine\"\n",
        "query= f\"Details of all the single patients present with respect to the {drug}\"\n",
        "query = query.replace(\"{drug}\", drug)\n",
        "# Prepare the input with a selected chunk of text\n",
        "context = get_context(chunks,query)\n",
        "system_text= f\"\"\" You are a literature analyst with the ability to understand the provided context and answer the queries related to patient information from the context only.\n",
        "        Follow the rules strictly.\n",
        "\n",
        "        Rules:\n",
        "        1. Don't hallucinate.\n",
        "        2. You must follow the Response format.\n",
        "        3. You must not display any thought process of yours in the response.\n",
        "        4. Extract all patient-related information such as age, gender, and other relevant details from the input text.\n",
        "\n",
        "        Queries:\n",
        "        - Provide patient information such as age, gender, and any other relevant details mentioned in the text.\n",
        "\n",
        "        Please use the provided input to give the response with respect to the above rules.\n",
        "\n",
        "        Carefully differentiate between patients and authors—do not mix up their names. Do not repeat patients in the total count or when providing their information. Also, differentiate each patient. If one patient is mentioned in multiple places, do not consider them as different persons. Do not consider patients present in table format data. Remember to check if the patient information is from table format data and exclude such cases.\n",
        "        Consider only patients related to {drug}. Do not consider any other patients.\n",
        "        Please determine if the patient is human and related to {drug}. If the patient is human, under the heading '# Patient Validation' give the answer as Valid. If not, give the answer as Invalid. There should be only Valid or Invalid present in the answer, nothing else.\n",
        "        (Only consider single patients like '39-Year-Old Male', '39-Year-Old Female', and strictly not groups of patients like '204 Patients', '6 patients', etc.). In the subsequent line, under the heading '# Total Number of Patients', provide the total number of patients present who are related to {drug}. If no single patient is present, give the answer as 0.\n",
        "        In the last line, under the heading '# Patient Type', list all the patients and include small and concise information about them who are related to {drug}. Include information on all cases of patients in the above text. If no single patient is present, give the answer as 'No patient present.'\n",
        "        Ensure that all information is accurate and based on factual data present in the text. Do not provide hallucinated responses or give answers that are not present in the text.\n",
        "\"\"\"\n",
        "system_text = system_text.replace(\"{drug}\", drug)\n",
        "chat_template = \"\"\"system\n",
        "\n",
        "{SYSTEM}user\n",
        "\n",
        "{INPUT}assistant\n",
        "\n",
        "{OUTPUT}\"\"\"\n",
        "\n",
        "input_text = chat_template.format(\n",
        "    SYSTEM=system_text,\n",
        "    INPUT=f\"context: {context} Question: {query}\",\n",
        "    OUTPUT=\"\"\n",
        ")\n",
        "patient_details=result(input_text, model,tokenizer)\n",
        "# Print the result\n",
        "patient_details[0]\n"
      ],
      "metadata": {
        "id": "ivqRSrJPKNek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure it's a string by accessing the first element if it's a list\n",
        "if isinstance(patient_details, list):\n",
        "    patient_details = patient_details[0]\n",
        "\n",
        "patientDetails = patient_details.split(\"assistant\")\n",
        "print(patientDetails[1])"
      ],
      "metadata": {
        "id": "3f3qilc3fcmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip3 install -U xformers --index-url https://download.pytorch.org/whl/cu121\n",
        "# !pip3 install -U trl<0.9.0 --index-url https://download.pytorch.org/whl/cu121\n",
        "# !pip3 install -U peft --index-url https://download.pytorch.org/whl/cu121\n",
        "# !pip3 install -U accelerate --index-url https://download.pytorch.org/whl/cu121\n",
        "# !pip3 install -U bitsandbytes --index-url https://download.pytorch.org/whl/cu121"
      ],
      "metadata": {
        "id": "F_D9l4GOKNaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Vdi8sHSbKNXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4wIZEGNkKNVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ugOmF81fKNSn"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "private_outputs": true,
      "cell_execution_strategy": "setup"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}